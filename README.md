# Mohamed RAG API

This project exposes a FastAPI-based REST API deployed on **Google Cloud Run**. It allows users to ask questions to a personalized assistant named **Mohamed**, powered by a combination of **Retrieval-Augmented Generation (RAG)** and a **Large Language Model (LLM)** to provide accurate, contextual answers.

---

## ðŸ“š Features

- ðŸ” **Document retrieval** using **ChromaDB** (vector database)
- ðŸ§  **RAG logic** to answer using relevant retrieved documents
- ðŸ—£ï¸ **Fallback to prompt-only mode** using a pre-defined FAQ if no relevant documents are found
- ðŸ¤– **LLM integration** (DeepSeek V3 0324) for human-like response generation
- ðŸŒ Exposed as a **REST API** using FastAPI
- ðŸ³ Docker + Makefile setup for local runs and cloud deployment
- ðŸ”§ Environment variables handled via `.env` file

---

## âš™ï¸ Architecture

```mermaid
flowchart TD
    Q[User Question] --> RAG{Is RAG relevant?}
    RAG -- Yes --> PromptRAG[â†’ Build context-based prompt from documents]
    RAG -- No --> PromptOnly[â†’ Build generic prompt from local JSON FAQ]
    PromptRAG & PromptOnly --> LLM[Call the LLM]
    LLM --> Response
